{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import random #teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'cassandra'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up fields\n",
    "BODY = Field()\n",
    "NAME = Field()\n",
    "\n",
    "fields = {'name': ('name', NAME), 'body': ('body', BODY)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from json\n",
    "train, test = TabularDataset.splits(\n",
    "                path = 'data',\n",
    "                train = f'{PROJECT_NAME}_train.json',\n",
    "                test = f'{PROJECT_NAME}_test.json',\n",
    "                format = 'json',\n",
    "                fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'name': <torchtext.data.field.Field object at 0x7f9e631b8438>, 'body': <torchtext.data.field.Field object at 0x7f9e631b8940>}\n",
      "len(train) 11490\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY.build_vocab(train.body, train.name)\n",
    "NAME.build_vocab(train.body, train.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12236\n",
      "12236\n"
     ]
    }
   ],
   "source": [
    "print(len(BODY.vocab))\n",
    "print(len(NAME.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', 94167), (')', 94167), ('.', 67730), (';', 55698), (',', 46923), ('{', 20473), ('}', 20473), ('=', 18020), ('get', 11897), ('<sentence_start>', 11490)]\n"
     ]
    }
   ],
   "source": [
    "print(BODY.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make iterator for splits\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.name),\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFeatures(nn.Module):\n",
    "    \"\"\"\n",
    "    Page 3 of the paper\n",
    "    attention_features (code tokens c, context h_{t-1})\n",
    "     C <- lookupandpad(c, E)\n",
    "     L1 <- ReLU(Conv1d(C, K_{l1}))\n",
    "     L2 <- Conv1d(L1, K_{l2}) * h_{t-1}\n",
    "     Lfeat <- L2/||L2||_2\n",
    "     return Lfeat\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, k1, w1, k2, w2, w3, dropout, prelu):\n",
    "        super(AttentionFeatures, self).__init__()\n",
    "                \n",
    "        self.w1 = w1\n",
    "        self.k1 = k1\n",
    "\n",
    "        self.w2 = w2\n",
    "        self.k2 = k2\n",
    "\n",
    "        #self.w3 = w3 #use this to calculate padding\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, k1, w1)\n",
    "        self.conv2 = nn.Conv1d(k1, k2, w2)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.relu = nn.PReLU() if prelu == True else F.relu\n",
    "\n",
    "    def forward(self, C, h_t):\n",
    "        \n",
    "        #C = embedded body tokens\n",
    "        #h_t = previous hidden state used to predict name token\n",
    "        \n",
    "        #C = [bodies len, batch size, emb dim]\n",
    "        #h_t = [1, batch size, k2]\n",
    "        \n",
    "        C = C.permute(1, 2, 0) #input to conv needs n_channels as dim 1\n",
    "        \n",
    "        #C = [batch size, emb dim, bodies len]\n",
    "        \n",
    "        h_t = h_t.permute(1, 2, 0) #from [1, batch size, k2] to [batch size, k2, 1]\n",
    "        \n",
    "        #h_t = [batch size, k2, 1]\n",
    "        \n",
    "        L_1 = self.do(self.relu(self.conv1(C)))\n",
    "        \n",
    "        #L_1 = [batch size, k1, bodies len - w1 + 1]\n",
    "        \n",
    "        L_2 = self.do(self.conv2(L_1)) * h_t\n",
    "                \n",
    "        #L_2 = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "        \n",
    "        L_feat = F.normalize(L_2, p=2, dim=1)\n",
    "                \n",
    "        #L_feat = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "                \n",
    "        return L_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeights(nn.Module):\n",
    "    \"\"\"\n",
    "    Page 3 of the paper\n",
    "    attention_features (attention features Lfeat, kernel K)\n",
    "     return Softmax(Conv1d(Lfeat, K))\n",
    "    \"\"\"\n",
    "    def __init__(self, k2, w3, dropout):\n",
    "        super(AttentionWeights, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(k2, 1, w3)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, L_feat):\n",
    "                \n",
    "        #L_feat = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "        \n",
    "        x = self.do(self.conv1(L_feat))\n",
    "        \n",
    "        #x = [batch size, 1, bodies len - w1 - w2 - w3 + 3]\n",
    "        \n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        #x = [batch size, bodies len - w1 - w2 - w3 + 3]\n",
    "        \n",
    "        x = F.softmax(x, dim=1)\n",
    "                \n",
    "        #x = [batch size, bodies len - w1 - w2 - w3 + 3]\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttentionNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, k1, k2, w1, w2, w3, dropout, prelu, pad_idx):\n",
    "        super(ConvAttentionNetwork, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.w3 = w3\n",
    "        self.dropout = dropout\n",
    "        self.prelu = prelu\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, k2)\n",
    "        self.attn_feat = AttentionFeatures(embedding_dim, k1, w1, k2, w2, w3, dropout, prelu)\n",
    "        self.attn_weights = AttentionWeights(k2, w3, dropout)\n",
    "        self.bias = nn.Parameter(torch.ones(vocab_size))\n",
    "        \n",
    "        n_padding = w1 + w2 + w3 - 3\n",
    "        self.padding = torch.zeros(n_padding, 1).fill_(pad_idx).long()\n",
    "        \n",
    "    def forward(self, bodies, names):\n",
    "        \n",
    "        #bodies = [bodies len, batch size]\n",
    "        #names = [names len, batch size]  \n",
    "        \n",
    "        #stores the probabilities generated for each token\n",
    "        outputs = torch.zeros(names.shape[0], names.shape[1], self.vocab_size).to(names.device)\n",
    "        \n",
    "        #outputs = [name len, batch size, vocab dim]\n",
    "        \n",
    "        bodies_padded = torch.cat((bodies, self.padding.expand(-1, bodies.shape[1]).to(bodies.device)))\n",
    "        \n",
    "        #bodies_padded = [bodies len + w1 + w2 + w3 - 3, batch_size]\n",
    "        \n",
    "        emb_b = self.embedding(bodies_padded)\n",
    "        \n",
    "        #emb_b = [bodies len, batch size, emb dim]\n",
    "                \n",
    "        #first input to gru is <sos> token\n",
    "        output = names[0]\n",
    "            \n",
    "        for i in range(1, names.shape[0]):\n",
    "            \n",
    "            #initial hidden state is rnn applied to the <sos> token\n",
    "            _, h_t = self.gru(self.embedding(output).unsqueeze(0))\n",
    "\n",
    "            #h_t = [1, batch size, k2]\n",
    "\n",
    "            L_feat = self.attn_feat(emb_b, h_t)\n",
    "\n",
    "            #L_feat = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "\n",
    "            alpha = self.attn_weights(L_feat)\n",
    "\n",
    "            #alpha = [batch size, bodies len - w1 - w2 - w3 + 3]\n",
    "\n",
    "            emb_b_slice = emb_b.permute(1, 0, 2)[:, :bodies.shape[0], :]\n",
    "\n",
    "            #emb_b = [batch_size, bodies len, emb dim]\n",
    "\n",
    "            n_hat = torch.sum(alpha.unsqueeze(2) * emb_b_slice, dim=1)\n",
    "\n",
    "            #n_hat = [batch size, emb dim]\n",
    "\n",
    "            E = self.embedding.weight.unsqueeze(0).expand(bodies.shape[1],-1,-1)\n",
    "\n",
    "            #E = [batch size, vocab size, emb dim]\n",
    "\n",
    "            n = torch.bmm(E, n_hat.unsqueeze(2)).squeeze(2) + self.bias.unsqueeze(0).expand(bodies.shape[1], -1)\n",
    "            \n",
    "            #n = [batch size, vocab size]\n",
    "            \n",
    "            outputs[i] = n\n",
    "            \n",
    "            #teacher forcing ratio is equal to dropout\n",
    "            if random.random() < self.dropout:\n",
    "                \n",
    "                top1 = n.max(1)[1]\n",
    "                output = top1\n",
    "                \n",
    "            else:\n",
    "                output = names[i]\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(BODY.vocab)\n",
    "EMBEDDING_DIM = 128\n",
    "K1 = 8\n",
    "K2 = 8\n",
    "W1 = 24\n",
    "W2 = 29\n",
    "W3 = 10\n",
    "DROPOUT = 0.5\n",
    "PRELU = True\n",
    "PAD_IDX = BODY.vocab.stoi['<pad>']\n",
    "\n",
    "model = ConvAttentionNetwork(VOCAB_SIZE, EMBEDDING_DIM, K1, K2, W1, W2, W3, DROPOUT, PRELU, PAD_IDX)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize optimizer, scheduler and loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        bodies = batch.body\n",
    "        names = batch.name\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(bodies, names)\n",
    "        \n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), names[1:].view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            bodies = batch.body\n",
    "            names = batch.name\n",
    "\n",
    "            output = model(bodies, names)\n",
    "\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), names[1:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.conda/envs/pytorch04/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 9.204 | Train PPL:  9933.65 | Test Loss: 8.354 | Test PPL:  4246.63 |\n",
      "| Epoch: 02 | Train Loss: 8.399 | Train PPL:  4442.23 | Test Loss: 7.298 | Test PPL:  1477.10 |\n",
      "| Epoch: 03 | Train Loss: 7.812 | Train PPL:  2470.23 | Test Loss: 7.042 | Test PPL:  1144.12 |\n",
      "| Epoch: 04 | Train Loss: 7.349 | Train PPL:  1554.21 | Test Loss: 6.719 | Test PPL:   828.37 |\n",
      "| Epoch: 05 | Train Loss: 7.007 | Train PPL:  1104.33 | Test Loss: 6.518 | Test PPL:   677.49 |\n",
      "| Epoch: 06 | Train Loss: 6.705 | Train PPL:   816.36 | Test Loss: 6.202 | Test PPL:   493.71 |\n",
      "| Epoch: 07 | Train Loss: 6.471 | Train PPL:   646.36 | Test Loss: 6.218 | Test PPL:   501.84 |\n",
      "| Epoch: 08 | Train Loss: 6.217 | Train PPL:   501.24 | Test Loss: 5.735 | Test PPL:   309.63 |\n",
      "| Epoch: 09 | Train Loss: 6.068 | Train PPL:   431.70 | Test Loss: 6.565 | Test PPL:   709.96 |\n",
      "| Epoch: 10 | Train Loss: 5.927 | Train PPL:   375.03 | Test Loss: 5.140 | Test PPL:   170.72 |\n",
      "| Epoch: 11 | Train Loss: 5.741 | Train PPL:   311.26 | Test Loss: 5.063 | Test PPL:   158.12 |\n",
      "| Epoch: 12 | Train Loss: 5.582 | Train PPL:   265.63 | Test Loss: 4.857 | Test PPL:   128.65 |\n",
      "| Epoch: 13 | Train Loss: 5.492 | Train PPL:   242.69 | Test Loss: 4.970 | Test PPL:   144.01 |\n",
      "| Epoch: 14 | Train Loss: 5.378 | Train PPL:   216.63 | Test Loss: 4.783 | Test PPL:   119.44 |\n",
      "| Epoch: 15 | Train Loss: 5.264 | Train PPL:   193.16 | Test Loss: 4.717 | Test PPL:   111.79 |\n",
      "| Epoch: 16 | Train Loss: 5.136 | Train PPL:   169.95 | Test Loss: 4.643 | Test PPL:   103.86 |\n",
      "| Epoch: 17 | Train Loss: 5.080 | Train PPL:   160.78 | Test Loss: 4.458 | Test PPL:    86.32 |\n",
      "| Epoch: 18 | Train Loss: 5.001 | Train PPL:   148.50 | Test Loss: 4.276 | Test PPL:    71.98 |\n",
      "| Epoch: 19 | Train Loss: 4.901 | Train PPL:   134.46 | Test Loss: 4.257 | Test PPL:    70.62 |\n",
      "| Epoch: 20 | Train Loss: 4.808 | Train PPL:   122.52 | Test Loss: 4.237 | Test PPL:    69.21 |\n",
      "| Epoch: 21 | Train Loss: 4.794 | Train PPL:   120.77 | Test Loss: 4.119 | Test PPL:    61.47 |\n",
      "| Epoch: 22 | Train Loss: 4.700 | Train PPL:   109.90 | Test Loss: 4.104 | Test PPL:    60.60 |\n",
      "| Epoch: 23 | Train Loss: 4.651 | Train PPL:   104.64 | Test Loss: 4.061 | Test PPL:    58.02 |\n",
      "| Epoch: 24 | Train Loss: 4.606 | Train PPL:   100.13 | Test Loss: 4.053 | Test PPL:    57.57 |\n",
      "| Epoch: 25 | Train Loss: 4.552 | Train PPL:    94.79 | Test Loss: 4.051 | Test PPL:    57.46 |\n",
      "| Epoch: 26 | Train Loss: 4.475 | Train PPL:    87.79 | Test Loss: 3.967 | Test PPL:    52.85 |\n",
      "| Epoch: 27 | Train Loss: 4.415 | Train PPL:    82.66 | Test Loss: 3.937 | Test PPL:    51.28 |\n",
      "| Epoch: 28 | Train Loss: 4.362 | Train PPL:    78.38 | Test Loss: 3.951 | Test PPL:    52.01 |\n",
      "| Epoch: 29 | Train Loss: 4.346 | Train PPL:    77.19 | Test Loss: 3.874 | Test PPL:    48.12 |\n",
      "| Epoch: 30 | Train Loss: 4.304 | Train PPL:    73.97 | Test Loss: 3.853 | Test PPL:    47.12 |\n",
      "| Epoch: 31 | Train Loss: 4.272 | Train PPL:    71.67 | Test Loss: 3.862 | Test PPL:    47.56 |\n",
      "| Epoch: 32 | Train Loss: 4.225 | Train PPL:    68.37 | Test Loss: 3.773 | Test PPL:    43.52 |\n",
      "| Epoch: 33 | Train Loss: 4.172 | Train PPL:    64.82 | Test Loss: 3.765 | Test PPL:    43.18 |\n",
      "| Epoch: 34 | Train Loss: 4.134 | Train PPL:    62.41 | Test Loss: 3.789 | Test PPL:    44.20 |\n",
      "| Epoch: 35 | Train Loss: 4.086 | Train PPL:    59.48 | Test Loss: 3.778 | Test PPL:    43.74 |\n",
      "| Epoch: 36 | Train Loss: 4.059 | Train PPL:    57.92 | Test Loss: 3.742 | Test PPL:    42.18 |\n",
      "| Epoch: 37 | Train Loss: 4.042 | Train PPL:    56.97 | Test Loss: 3.727 | Test PPL:    41.55 |\n",
      "| Epoch: 38 | Train Loss: 4.004 | Train PPL:    54.80 | Test Loss: 3.740 | Test PPL:    42.11 |\n",
      "| Epoch: 39 | Train Loss: 3.979 | Train PPL:    53.44 | Test Loss: 3.683 | Test PPL:    39.77 |\n",
      "| Epoch: 40 | Train Loss: 3.934 | Train PPL:    51.11 | Test Loss: 3.699 | Test PPL:    40.42 |\n",
      "| Epoch: 41 | Train Loss: 3.922 | Train PPL:    50.52 | Test Loss: 3.666 | Test PPL:    39.09 |\n",
      "| Epoch: 42 | Train Loss: 3.895 | Train PPL:    49.15 | Test Loss: 3.668 | Test PPL:    39.17 |\n",
      "| Epoch: 43 | Train Loss: 3.847 | Train PPL:    46.87 | Test Loss: 3.652 | Test PPL:    38.55 |\n",
      "| Epoch: 44 | Train Loss: 3.829 | Train PPL:    46.00 | Test Loss: 3.662 | Test PPL:    38.93 |\n",
      "| Epoch: 45 | Train Loss: 3.804 | Train PPL:    44.90 | Test Loss: 3.638 | Test PPL:    38.01 |\n",
      "| Epoch: 46 | Train Loss: 3.786 | Train PPL:    44.09 | Test Loss: 3.611 | Test PPL:    37.02 |\n",
      "| Epoch: 47 | Train Loss: 3.761 | Train PPL:    43.00 | Test Loss: 3.664 | Test PPL:    39.01 |\n",
      "| Epoch: 48 | Train Loss: 3.762 | Train PPL:    43.03 | Test Loss: 3.623 | Test PPL:    37.43 |\n",
      "| Epoch: 49 | Train Loss: 3.745 | Train PPL:    42.29 | Test Loss: 3.601 | Test PPL:    36.62 |\n",
      "| Epoch: 50 | Train Loss: 3.699 | Train PPL:    40.42 | Test Loss: 3.601 | Test PPL:    36.65 |\n",
      "| Epoch: 51 | Train Loss: 3.678 | Train PPL:    39.56 | Test Loss: 3.599 | Test PPL:    36.58 |\n",
      "| Epoch: 52 | Train Loss: 3.684 | Train PPL:    39.82 | Test Loss: 3.591 | Test PPL:    36.27 |\n",
      "| Epoch: 53 | Train Loss: 3.657 | Train PPL:    38.73 | Test Loss: 3.592 | Test PPL:    36.32 |\n",
      "| Epoch: 54 | Train Loss: 3.632 | Train PPL:    37.81 | Test Loss: 3.576 | Test PPL:    35.74 |\n",
      "| Epoch: 55 | Train Loss: 3.624 | Train PPL:    37.50 | Test Loss: 3.572 | Test PPL:    35.58 |\n",
      "| Epoch: 56 | Train Loss: 3.595 | Train PPL:    36.42 | Test Loss: 3.573 | Test PPL:    35.63 |\n",
      "| Epoch: 57 | Train Loss: 3.584 | Train PPL:    36.02 | Test Loss: 3.527 | Test PPL:    34.00 |\n",
      "| Epoch: 58 | Train Loss: 3.565 | Train PPL:    35.33 | Test Loss: 3.528 | Test PPL:    34.04 |\n",
      "| Epoch: 59 | Train Loss: 3.542 | Train PPL:    34.53 | Test Loss: 3.534 | Test PPL:    34.28 |\n",
      "| Epoch: 60 | Train Loss: 3.512 | Train PPL:    33.51 | Test Loss: 3.519 | Test PPL:    33.74 |\n",
      "| Epoch: 61 | Train Loss: 3.516 | Train PPL:    33.64 | Test Loss: 3.498 | Test PPL:    33.03 |\n",
      "| Epoch: 62 | Train Loss: 3.499 | Train PPL:    33.09 | Test Loss: 3.525 | Test PPL:    33.95 |\n",
      "| Epoch: 63 | Train Loss: 3.475 | Train PPL:    32.29 | Test Loss: 3.469 | Test PPL:    32.11 |\n",
      "| Epoch: 64 | Train Loss: 3.454 | Train PPL:    31.62 | Test Loss: 3.481 | Test PPL:    32.48 |\n",
      "| Epoch: 65 | Train Loss: 3.457 | Train PPL:    31.73 | Test Loss: 3.463 | Test PPL:    31.92 |\n",
      "| Epoch: 66 | Train Loss: 3.417 | Train PPL:    30.47 | Test Loss: 3.452 | Test PPL:    31.57 |\n",
      "| Epoch: 67 | Train Loss: 3.420 | Train PPL:    30.56 | Test Loss: 3.450 | Test PPL:    31.50 |\n",
      "| Epoch: 68 | Train Loss: 3.399 | Train PPL:    29.92 | Test Loss: 3.446 | Test PPL:    31.38 |\n",
      "| Epoch: 69 | Train Loss: 3.398 | Train PPL:    29.89 | Test Loss: 3.451 | Test PPL:    31.52 |\n",
      "| Epoch: 70 | Train Loss: 3.380 | Train PPL:    29.37 | Test Loss: 3.424 | Test PPL:    30.68 |\n",
      "| Epoch: 71 | Train Loss: 3.366 | Train PPL:    28.96 | Test Loss: 3.439 | Test PPL:    31.16 |\n",
      "| Epoch: 72 | Train Loss: 3.361 | Train PPL:    28.83 | Test Loss: 3.428 | Test PPL:    30.83 |\n",
      "| Epoch: 73 | Train Loss: 3.342 | Train PPL:    28.29 | Test Loss: 3.427 | Test PPL:    30.78 |\n",
      "| Epoch: 74 | Train Loss: 3.326 | Train PPL:    27.81 | Test Loss: 3.414 | Test PPL:    30.39 |\n",
      "| Epoch: 75 | Train Loss: 3.309 | Train PPL:    27.35 | Test Loss: 3.395 | Test PPL:    29.80 |\n",
      "| Epoch: 76 | Train Loss: 3.294 | Train PPL:    26.95 | Test Loss: 3.403 | Test PPL:    30.05 |\n",
      "| Epoch: 77 | Train Loss: 3.294 | Train PPL:    26.95 | Test Loss: 3.405 | Test PPL:    30.11 |\n",
      "| Epoch: 78 | Train Loss: 3.258 | Train PPL:    26.00 | Test Loss: 3.390 | Test PPL:    29.66 |\n",
      "| Epoch: 79 | Train Loss: 3.241 | Train PPL:    25.56 | Test Loss: 3.365 | Test PPL:    28.94 |\n",
      "| Epoch: 80 | Train Loss: 3.231 | Train PPL:    25.31 | Test Loss: 3.380 | Test PPL:    29.38 |\n",
      "| Epoch: 81 | Train Loss: 3.220 | Train PPL:    25.03 | Test Loss: 3.370 | Test PPL:    29.09 |\n",
      "| Epoch: 82 | Train Loss: 3.197 | Train PPL:    24.47 | Test Loss: 3.368 | Test PPL:    29.02 |\n",
      "| Epoch: 83 | Train Loss: 3.205 | Train PPL:    24.64 | Test Loss: 3.340 | Test PPL:    28.22 |\n",
      "| Epoch: 84 | Train Loss: 3.178 | Train PPL:    24.01 | Test Loss: 3.371 | Test PPL:    29.11 |\n",
      "| Epoch: 85 | Train Loss: 3.161 | Train PPL:    23.60 | Test Loss: 3.333 | Test PPL:    28.03 |\n",
      "| Epoch: 86 | Train Loss: 3.161 | Train PPL:    23.58 | Test Loss: 3.340 | Test PPL:    28.22 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 87 | Train Loss: 3.149 | Train PPL:    23.31 | Test Loss: 3.304 | Test PPL:    27.21 |\n",
      "| Epoch: 88 | Train Loss: 3.144 | Train PPL:    23.19 | Test Loss: 3.306 | Test PPL:    27.28 |\n",
      "| Epoch: 89 | Train Loss: 3.114 | Train PPL:    22.51 | Test Loss: 3.301 | Test PPL:    27.14 |\n",
      "| Epoch: 90 | Train Loss: 3.097 | Train PPL:    22.13 | Test Loss: 3.289 | Test PPL:    26.81 |\n",
      "| Epoch: 91 | Train Loss: 3.108 | Train PPL:    22.37 | Test Loss: 3.292 | Test PPL:    26.88 |\n",
      "| Epoch: 92 | Train Loss: 3.096 | Train PPL:    22.11 | Test Loss: 3.260 | Test PPL:    26.05 |\n",
      "| Epoch: 93 | Train Loss: 3.076 | Train PPL:    21.67 | Test Loss: 3.286 | Test PPL:    26.72 |\n",
      "| Epoch: 94 | Train Loss: 3.075 | Train PPL:    21.64 | Test Loss: 3.266 | Test PPL:    26.21 |\n",
      "| Epoch: 95 | Train Loss: 3.045 | Train PPL:    21.01 | Test Loss: 3.264 | Test PPL:    26.16 |\n",
      "| Epoch: 96 | Train Loss: 3.050 | Train PPL:    21.11 | Test Loss: 3.230 | Test PPL:    25.29 |\n",
      "| Epoch: 97 | Train Loss: 3.027 | Train PPL:    20.64 | Test Loss: 3.250 | Test PPL:    25.80 |\n",
      "| Epoch: 98 | Train Loss: 3.014 | Train PPL:    20.36 | Test Loss: 3.249 | Test PPL:    25.76 |\n",
      "| Epoch: 99 | Train Loss: 3.022 | Train PPL:    20.52 | Test Loss: 3.242 | Test PPL:    25.59 |\n",
      "| Epoch: 100 | Train Loss: 2.992 | Train PPL:    19.92 | Test Loss: 3.243 | Test PPL:    25.61 |\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "N_EPOCHS = 100\n",
    "CLIP = 10\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir('.save'):\n",
    "    os.makedirs('.save')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    test_loss = evaluate(model, test_iter, criterion)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), '.save/model.pt')\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):8.2f} | Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):8.2f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
