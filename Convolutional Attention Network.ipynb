{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import random #teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'cassandra'\n",
    "EMBEDDING_INIT = 'glove200'\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up fields\n",
    "BODY = Field()\n",
    "NAME = Field()\n",
    "\n",
    "fields = {'name': ('name', NAME), 'body': ('body', BODY)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from json\n",
    "train, test = TabularDataset.splits(\n",
    "                path = 'data',\n",
    "                train = f'{PROJECT_NAME}_train.json',\n",
    "                test = f'{PROJECT_NAME}_test.json',\n",
    "                format = 'json',\n",
    "                fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'name': <torchtext.data.field.Field object at 0x7f3a0f4c06a0>, 'body': <torchtext.data.field.Field object at 0x7f3a0f4c0ba8>}\n",
      "len(train) 11490\n"
     ]
    }
   ],
   "source": [
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BODY.build_vocab(train.body, train.name, vectors=\"glove.6B.200d\")\n",
    "NAME.build_vocab(train.body, train.name, vectors=\"glove.6B.200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12236\n",
      "12236\n"
     ]
    }
   ],
   "source": [
    "print(len(BODY.vocab))\n",
    "print(len(NAME.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', 94167), (')', 94167), ('.', 67730), (';', 55698), (',', 46923), ('{', 20473), ('}', 20473), ('=', 18020), ('get', 11897), ('<sentence_start>', 11490)]\n"
     ]
    }
   ],
   "source": [
    "print(BODY.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make iterator for splits\n",
    "train_iter, test_iter = BucketIterator.splits(\n",
    "    (train, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.name),\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFeatures(nn.Module):\n",
    "    \"\"\"\n",
    "    Page 3 of the paper\n",
    "    attention_features (code tokens c, context h_{t-1})\n",
    "     C <- lookupandpad(c, E)\n",
    "     L1 <- ReLU(Conv1d(C, K_{l1}))\n",
    "     L2 <- Conv1d(L1, K_{l2}) * h_{t-1}\n",
    "     Lfeat <- L2/||L2||_2\n",
    "     return Lfeat\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, k1, w1, k2, w2, w3, dropout, prelu):\n",
    "        super(AttentionFeatures, self).__init__()\n",
    "                \n",
    "        self.w1 = w1\n",
    "        self.k1 = k1\n",
    "\n",
    "        self.w2 = w2\n",
    "        self.k2 = k2\n",
    "\n",
    "        #self.w3 = w3 #use this to calculate padding\n",
    "\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, k1, w1)\n",
    "        self.conv2 = nn.Conv1d(k1, k2, w2)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.relu = nn.PReLU() if prelu == True else F.relu\n",
    "\n",
    "    def forward(self, C, h_t):\n",
    "        \n",
    "        #C = embedded body tokens\n",
    "        #h_t = previous hidden state used to predict name token\n",
    "        \n",
    "        #C = [bodies len, batch size, emb dim]\n",
    "        #h_t = [1, batch size, k2]\n",
    "        \n",
    "        C = C.permute(1, 2, 0) #input to conv needs n_channels as dim 1\n",
    "        \n",
    "        #C = [batch size, emb dim, bodies len]\n",
    "        \n",
    "        h_t = h_t.permute(1, 2, 0) #from [1, batch size, k2] to [batch size, k2, 1]\n",
    "        \n",
    "        #h_t = [batch size, k2, 1]\n",
    "        \n",
    "        L_1 = self.do(self.relu(self.conv1(C)))\n",
    "        \n",
    "        #L_1 = [batch size, k1, bodies len - w1 + 1]\n",
    "        \n",
    "        L_2 = self.do(self.conv2(L_1)) * h_t\n",
    "                \n",
    "        #L_2 = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "        \n",
    "        L_feat = F.normalize(L_2, p=2, dim=1)\n",
    "                \n",
    "        #L_feat = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "                \n",
    "        return L_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeights(nn.Module):\n",
    "    \"\"\"\n",
    "    Page 3 of the paper\n",
    "    attention_features (attention features Lfeat, kernel K)\n",
    "     return Softmax(Conv1d(Lfeat, K))\n",
    "    \"\"\"\n",
    "    def __init__(self, k2, w3, dropout):\n",
    "        super(AttentionWeights, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(k2, 1, w3)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, L_feat):\n",
    "                \n",
    "        #L_feat = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "        \n",
    "        x = self.do(self.conv1(L_feat))\n",
    "        \n",
    "        #x = [batch size, 1, bodies len - w1 - w2 - w3 + 3]\n",
    "        \n",
    "        x = x.squeeze(1)\n",
    "        \n",
    "        #x = [batch size, bodies len - w1 - w2 - w3 + 3]\n",
    "        \n",
    "        x = F.softmax(x, dim=1)\n",
    "                \n",
    "        #x = [batch size, bodies len - w1 - w2 - w3 + 3]\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttentionNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, k1, k2, w1, w2, w3, dropout, prelu, pad_idx):\n",
    "        super(ConvAttentionNetwork, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.w3 = w3\n",
    "        self.dropout = dropout\n",
    "        self.prelu = prelu\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, k2)\n",
    "        self.attn_feat = AttentionFeatures(embedding_dim, k1, w1, k2, w2, w3, dropout, prelu)\n",
    "        self.attn_weights = AttentionWeights(k2, w3, dropout)\n",
    "        self.bias = nn.Parameter(torch.ones(vocab_size))\n",
    "        \n",
    "        n_padding = w1 + w2 + w3 - 3\n",
    "        self.padding = torch.zeros(n_padding, 1).fill_(pad_idx).long()\n",
    "        \n",
    "    def forward(self, bodies, names, tf=None):\n",
    "        \n",
    "        if tf is None:\n",
    "            tf = self.dropout\n",
    "        \n",
    "        #bodies = [bodies len, batch size]\n",
    "        #names = [names len, batch size]  \n",
    "        \n",
    "        #stores the probabilities generated for each token\n",
    "        outputs = torch.zeros(names.shape[0], names.shape[1], self.vocab_size).to(names.device)\n",
    "        \n",
    "        #outputs = [name len, batch size, vocab dim]\n",
    "        \n",
    "        #need to pad the function body so after it has been fed through\n",
    "        #the convolutional layers it is the same size as the original function body\n",
    "        bodies_padded = torch.cat((bodies, self.padding.expand(-1, bodies.shape[1]).to(bodies.device)))\n",
    "        \n",
    "        #bodies_padded = [bodies len + w1 + w2 + w3 - 3, batch_size]\n",
    "        \n",
    "        #from now on when we refer to bodies len, we mean the padded version\n",
    "        \n",
    "        #convert function body tokens into their embeddings\n",
    "        emb_b = self.embedding(bodies_padded)\n",
    "        \n",
    "        #emb_b = [bodies len, batch size, emb dim]\n",
    "                \n",
    "        #first input to the gru is the first token of the function name\n",
    "        #which is a start of sentence token\n",
    "        output = names[0]\n",
    "            \n",
    "        #generate predicted function name tokens one at a time\n",
    "        for i in range(1, names.shape[0]):\n",
    "                        \n",
    "            #initial hidden state is start of sentence token passed through gru\n",
    "            #subsequent hidden states from either the previous token predicted by the model\n",
    "            #or the ground truth token the model should have predicted\n",
    "            _, h_t = self.gru(self.embedding(output).unsqueeze(0))\n",
    "\n",
    "            #h_t = [1, batch size, k2]\n",
    "\n",
    "            #computes `k2` features for each token which are scaled by h_t\n",
    "            L_feat = self.attn_feat(emb_b, h_t)\n",
    "\n",
    "            #L_feat = [batch size, k2, bodies len - w1 - w2 + 2]\n",
    "\n",
    "            #computes the attention values for each token in the function body\n",
    "            #the second dimension is now equal to the original unpadded `bodies len` size\n",
    "            alpha = self.attn_weights(L_feat)\n",
    "\n",
    "            #alpha = [batch size, bodies len - w1 - w2 - w3 + 3]\n",
    "\n",
    "            #emb_b also contains the padding tokens so we slice these off\n",
    "            emb_b_slice = emb_b.permute(1, 0, 2)[:, :bodies.shape[0], :]\n",
    "\n",
    "            #emb_b = [batch_size, bodies len, emb dim]\n",
    "\n",
    "            #apply the attention to the embedded function body tokens\n",
    "            n_hat = torch.sum(alpha.unsqueeze(2) * emb_b_slice, dim=1)\n",
    "\n",
    "            #n_hat = [batch size, emb dim]\n",
    "\n",
    "            #E is the embedding layer weights\n",
    "            E = self.embedding.weight.unsqueeze(0).expand(bodies.shape[1],-1,-1)\n",
    "\n",
    "            #E = [batch size, vocab size, emb dim]\n",
    "\n",
    "            #matrix multiply E and n_hat and apply a bias\n",
    "            #n is the probability distribution over the vocabulary for the predicted next token\n",
    "            n = torch.bmm(E, n_hat.unsqueeze(2)).squeeze(2) + self.bias.unsqueeze(0).expand(bodies.shape[1], -1)\n",
    "            \n",
    "            #n = [batch size, vocab size]\n",
    "            \n",
    "            #store prediction probability distribution in large tensor that holds \n",
    "            #predictions for each token in the function name\n",
    "            outputs[i] = n\n",
    "            \n",
    "            #with probability of `tf`, use the model's prediction of the next token\n",
    "            #as the next token to feed into the model (to become the next h_t)\n",
    "            #with probability 1-`tf`, use the actual ground truth next token as\n",
    "            #the next token to feed into the model\n",
    "            #teacher forcing ratio is equal to dropout during training and 0 during inference\n",
    "            if random.random() < tf:\n",
    "                \n",
    "                #model's predicted token highest value in the probability distribution\n",
    "                top1 = n.max(1)[1]\n",
    "                output = top1\n",
    "                \n",
    "            else:\n",
    "                output = names[i]\n",
    "                \n",
    "        #outputs = [name len, batch size, vocab dim]\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(BODY.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "K1 = 8\n",
    "K2 = 8\n",
    "W1 = 24\n",
    "W2 = 29\n",
    "W3 = 10\n",
    "DROPOUT = 0.25\n",
    "PRELU = True\n",
    "PAD_IDX = BODY.vocab.stoi['<pad>']\n",
    "UNK_IDX = BODY.vocab.stoi['<unk>']\n",
    "\n",
    "model = ConvAttentionNetwork(VOCAB_SIZE, EMBEDDING_DIM, K1, K2, W1, W2, W3, DROPOUT, PRELU, PAD_IDX)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'glove' in EMBEDDING_INIT:\n",
    "    pretrained_embeddings = BODY.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize optimizer, scheduler and loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        bodies = batch.body\n",
    "        names = batch.name\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(bodies, names)\n",
    "        \n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), names[1:].view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            bodies = batch.body\n",
    "            names = batch.name\n",
    "\n",
    "            output = model(bodies, names, 0) #set teacher forcing to zero\n",
    "\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), names[1:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/.conda/envs/pytorch04/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 8.885 | Train PPL:  7225.99 | Test Loss: 8.692 | Test PPL:  5956.77 |\n",
      "| Epoch: 02 | Train Loss: 7.831 | Train PPL:  2516.65 | Test Loss: 7.344 | Test PPL:  1547.32 |\n",
      "| Epoch: 03 | Train Loss: 6.403 | Train PPL:   603.63 | Test Loss: 4.996 | Test PPL:   147.84 |\n",
      "| Epoch: 04 | Train Loss: 5.049 | Train PPL:   155.90 | Test Loss: 4.517 | Test PPL:    91.60 |\n",
      "| Epoch: 05 | Train Loss: 4.708 | Train PPL:   110.79 | Test Loss: 4.262 | Test PPL:    70.95 |\n",
      "| Epoch: 06 | Train Loss: 4.522 | Train PPL:    91.99 | Test Loss: 4.155 | Test PPL:    63.78 |\n",
      "| Epoch: 07 | Train Loss: 4.368 | Train PPL:    78.86 | Test Loss: 4.035 | Test PPL:    56.53 |\n",
      "| Epoch: 08 | Train Loss: 4.230 | Train PPL:    68.69 | Test Loss: 3.937 | Test PPL:    51.29 |\n",
      "| Epoch: 09 | Train Loss: 4.111 | Train PPL:    61.03 | Test Loss: 3.841 | Test PPL:    46.56 |\n",
      "| Epoch: 10 | Train Loss: 4.016 | Train PPL:    55.46 | Test Loss: 3.786 | Test PPL:    44.09 |\n",
      "| Epoch: 11 | Train Loss: 3.924 | Train PPL:    50.58 | Test Loss: 3.716 | Test PPL:    41.12 |\n",
      "| Epoch: 12 | Train Loss: 3.827 | Train PPL:    45.95 | Test Loss: 3.642 | Test PPL:    38.16 |\n",
      "| Epoch: 13 | Train Loss: 3.758 | Train PPL:    42.87 | Test Loss: 3.574 | Test PPL:    35.66 |\n",
      "| Epoch: 14 | Train Loss: 3.650 | Train PPL:    38.47 | Test Loss: 3.497 | Test PPL:    33.01 |\n",
      "| Epoch: 15 | Train Loss: 3.583 | Train PPL:    35.99 | Test Loss: 3.468 | Test PPL:    32.07 |\n",
      "| Epoch: 16 | Train Loss: 3.522 | Train PPL:    33.86 | Test Loss: 3.408 | Test PPL:    30.21 |\n",
      "| Epoch: 17 | Train Loss: 3.464 | Train PPL:    31.96 | Test Loss: 3.376 | Test PPL:    29.25 |\n",
      "| Epoch: 18 | Train Loss: 3.406 | Train PPL:    30.14 | Test Loss: 3.312 | Test PPL:    27.45 |\n",
      "| Epoch: 19 | Train Loss: 3.531 | Train PPL:    34.16 | Test Loss: 3.218 | Test PPL:    24.97 |\n",
      "| Epoch: 20 | Train Loss: 3.291 | Train PPL:    26.88 | Test Loss: 3.191 | Test PPL:    24.31 |\n",
      "| Epoch: 21 | Train Loss: 3.230 | Train PPL:    25.27 | Test Loss: 3.164 | Test PPL:    23.67 |\n",
      "| Epoch: 22 | Train Loss: 3.232 | Train PPL:    25.33 | Test Loss: 3.123 | Test PPL:    22.71 |\n",
      "| Epoch: 23 | Train Loss: 3.146 | Train PPL:    23.24 | Test Loss: 3.095 | Test PPL:    22.09 |\n",
      "| Epoch: 24 | Train Loss: 3.103 | Train PPL:    22.27 | Test Loss: 3.061 | Test PPL:    21.34 |\n",
      "| Epoch: 25 | Train Loss: 3.207 | Train PPL:    24.70 | Test Loss: 3.045 | Test PPL:    21.01 |\n",
      "| Epoch: 26 | Train Loss: 3.023 | Train PPL:    20.54 | Test Loss: 3.038 | Test PPL:    20.85 |\n",
      "| Epoch: 27 | Train Loss: 2.978 | Train PPL:    19.64 | Test Loss: 3.006 | Test PPL:    20.22 |\n",
      "| Epoch: 28 | Train Loss: 2.935 | Train PPL:    18.81 | Test Loss: 2.988 | Test PPL:    19.85 |\n",
      "| Epoch: 29 | Train Loss: 2.964 | Train PPL:    19.37 | Test Loss: 2.965 | Test PPL:    19.40 |\n",
      "| Epoch: 30 | Train Loss: 2.869 | Train PPL:    17.62 | Test Loss: 2.944 | Test PPL:    18.99 |\n",
      "| Epoch: 31 | Train Loss: 2.887 | Train PPL:    17.94 | Test Loss: 2.937 | Test PPL:    18.87 |\n",
      "| Epoch: 32 | Train Loss: 2.798 | Train PPL:    16.42 | Test Loss: 2.917 | Test PPL:    18.48 |\n",
      "| Epoch: 33 | Train Loss: 2.775 | Train PPL:    16.04 | Test Loss: 2.931 | Test PPL:    18.75 |\n",
      "| Epoch: 34 | Train Loss: 2.743 | Train PPL:    15.53 | Test Loss: 2.890 | Test PPL:    17.99 |\n",
      "| Epoch: 35 | Train Loss: 2.714 | Train PPL:    15.09 | Test Loss: 2.909 | Test PPL:    18.34 |\n",
      "| Epoch: 36 | Train Loss: 2.691 | Train PPL:    14.75 | Test Loss: 2.855 | Test PPL:    17.38 |\n",
      "| Epoch: 37 | Train Loss: 2.671 | Train PPL:    14.46 | Test Loss: 2.844 | Test PPL:    17.19 |\n",
      "| Epoch: 38 | Train Loss: 2.626 | Train PPL:    13.82 | Test Loss: 2.827 | Test PPL:    16.89 |\n",
      "| Epoch: 39 | Train Loss: 2.595 | Train PPL:    13.40 | Test Loss: 2.823 | Test PPL:    16.83 |\n",
      "| Epoch: 40 | Train Loss: 2.584 | Train PPL:    13.25 | Test Loss: 2.848 | Test PPL:    17.25 |\n",
      "| Epoch: 41 | Train Loss: 2.563 | Train PPL:    12.97 | Test Loss: 2.810 | Test PPL:    16.61 |\n",
      "| Epoch: 42 | Train Loss: 2.542 | Train PPL:    12.71 | Test Loss: 2.799 | Test PPL:    16.43 |\n",
      "| Epoch: 43 | Train Loss: 2.506 | Train PPL:    12.25 | Test Loss: 2.794 | Test PPL:    16.34 |\n",
      "| Epoch: 44 | Train Loss: 2.491 | Train PPL:    12.07 | Test Loss: 2.793 | Test PPL:    16.33 |\n",
      "| Epoch: 45 | Train Loss: 2.475 | Train PPL:    11.88 | Test Loss: 2.799 | Test PPL:    16.43 |\n",
      "| Epoch: 46 | Train Loss: 2.447 | Train PPL:    11.56 | Test Loss: 2.795 | Test PPL:    16.37 |\n",
      "| Epoch: 47 | Train Loss: 2.424 | Train PPL:    11.29 | Test Loss: 2.764 | Test PPL:    15.86 |\n",
      "| Epoch: 48 | Train Loss: 2.400 | Train PPL:    11.02 | Test Loss: 2.782 | Test PPL:    16.15 |\n",
      "| Epoch: 49 | Train Loss: 2.398 | Train PPL:    11.00 | Test Loss: 2.758 | Test PPL:    15.77 |\n",
      "| Epoch: 50 | Train Loss: 2.373 | Train PPL:    10.73 | Test Loss: 2.759 | Test PPL:    15.79 |\n",
      "| Epoch: 51 | Train Loss: 2.350 | Train PPL:    10.48 | Test Loss: 2.752 | Test PPL:    15.68 |\n",
      "| Epoch: 52 | Train Loss: 2.337 | Train PPL:    10.35 | Test Loss: 2.727 | Test PPL:    15.29 |\n",
      "| Epoch: 53 | Train Loss: 2.313 | Train PPL:    10.11 | Test Loss: 2.718 | Test PPL:    15.15 |\n",
      "| Epoch: 54 | Train Loss: 2.311 | Train PPL:    10.09 | Test Loss: 2.733 | Test PPL:    15.37 |\n",
      "| Epoch: 55 | Train Loss: 2.299 | Train PPL:     9.96 | Test Loss: 2.747 | Test PPL:    15.59 |\n",
      "| Epoch: 56 | Train Loss: 2.281 | Train PPL:     9.79 | Test Loss: 2.736 | Test PPL:    15.42 |\n",
      "| Epoch: 57 | Train Loss: 2.260 | Train PPL:     9.58 | Test Loss: 2.739 | Test PPL:    15.47 |\n",
      "| Epoch: 58 | Train Loss: 2.247 | Train PPL:     9.46 | Test Loss: 2.702 | Test PPL:    14.91 |\n",
      "| Epoch: 59 | Train Loss: 2.249 | Train PPL:     9.48 | Test Loss: 2.690 | Test PPL:    14.73 |\n",
      "| Epoch: 60 | Train Loss: 2.229 | Train PPL:     9.29 | Test Loss: 2.708 | Test PPL:    15.00 |\n",
      "| Epoch: 61 | Train Loss: 2.223 | Train PPL:     9.23 | Test Loss: 2.694 | Test PPL:    14.79 |\n",
      "| Epoch: 62 | Train Loss: 2.203 | Train PPL:     9.05 | Test Loss: 2.691 | Test PPL:    14.74 |\n",
      "| Epoch: 63 | Train Loss: 2.181 | Train PPL:     8.85 | Test Loss: 2.693 | Test PPL:    14.78 |\n",
      "| Epoch: 64 | Train Loss: 2.166 | Train PPL:     8.73 | Test Loss: 2.708 | Test PPL:    15.00 |\n",
      "| Epoch: 65 | Train Loss: 2.163 | Train PPL:     8.69 | Test Loss: 2.698 | Test PPL:    14.85 |\n",
      "| Epoch: 66 | Train Loss: 2.164 | Train PPL:     8.70 | Test Loss: 2.682 | Test PPL:    14.62 |\n",
      "| Epoch: 67 | Train Loss: 2.142 | Train PPL:     8.51 | Test Loss: 2.727 | Test PPL:    15.28 |\n",
      "| Epoch: 68 | Train Loss: 2.136 | Train PPL:     8.46 | Test Loss: 2.709 | Test PPL:    15.01 |\n",
      "| Epoch: 69 | Train Loss: 2.119 | Train PPL:     8.33 | Test Loss: 2.696 | Test PPL:    14.81 |\n",
      "| Epoch: 70 | Train Loss: 2.108 | Train PPL:     8.23 | Test Loss: 2.697 | Test PPL:    14.83 |\n",
      "| Epoch: 71 | Train Loss: 2.099 | Train PPL:     8.16 | Test Loss: 2.720 | Test PPL:    15.18 |\n",
      "| Epoch: 72 | Train Loss: 2.095 | Train PPL:     8.13 | Test Loss: 2.708 | Test PPL:    15.00 |\n",
      "| Epoch: 73 | Train Loss: 2.079 | Train PPL:     8.00 | Test Loss: 2.715 | Test PPL:    15.10 |\n",
      "| Epoch: 74 | Train Loss: 2.062 | Train PPL:     7.86 | Test Loss: 2.698 | Test PPL:    14.84 |\n",
      "| Epoch: 75 | Train Loss: 2.062 | Train PPL:     7.86 | Test Loss: 2.698 | Test PPL:    14.85 |\n",
      "| Epoch: 76 | Train Loss: 2.058 | Train PPL:     7.83 | Test Loss: 2.683 | Test PPL:    14.63 |\n",
      "| Epoch: 77 | Train Loss: 2.023 | Train PPL:     7.56 | Test Loss: 2.683 | Test PPL:    14.63 |\n",
      "| Epoch: 78 | Train Loss: 2.039 | Train PPL:     7.68 | Test Loss: 2.687 | Test PPL:    14.69 |\n",
      "| Epoch: 79 | Train Loss: 2.035 | Train PPL:     7.65 | Test Loss: 2.650 | Test PPL:    14.15 |\n",
      "| Epoch: 80 | Train Loss: 2.005 | Train PPL:     7.43 | Test Loss: 2.622 | Test PPL:    13.76 |\n",
      "| Epoch: 81 | Train Loss: 2.010 | Train PPL:     7.47 | Test Loss: 2.602 | Test PPL:    13.50 |\n",
      "| Epoch: 82 | Train Loss: 2.002 | Train PPL:     7.40 | Test Loss: 2.630 | Test PPL:    13.87 |\n",
      "| Epoch: 83 | Train Loss: 2.004 | Train PPL:     7.42 | Test Loss: 2.604 | Test PPL:    13.52 |\n",
      "| Epoch: 84 | Train Loss: 1.978 | Train PPL:     7.23 | Test Loss: 2.581 | Test PPL:    13.21 |\n",
      "| Epoch: 85 | Train Loss: 1.973 | Train PPL:     7.19 | Test Loss: 2.603 | Test PPL:    13.50 |\n",
      "| Epoch: 86 | Train Loss: 1.957 | Train PPL:     7.08 | Test Loss: 2.604 | Test PPL:    13.52 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 87 | Train Loss: 1.983 | Train PPL:     7.27 | Test Loss: 2.603 | Test PPL:    13.50 |\n",
      "| Epoch: 88 | Train Loss: 1.954 | Train PPL:     7.06 | Test Loss: 2.591 | Test PPL:    13.34 |\n",
      "| Epoch: 89 | Train Loss: 1.928 | Train PPL:     6.87 | Test Loss: 2.588 | Test PPL:    13.30 |\n",
      "| Epoch: 90 | Train Loss: 1.944 | Train PPL:     6.99 | Test Loss: 2.597 | Test PPL:    13.43 |\n",
      "| Epoch: 91 | Train Loss: 1.932 | Train PPL:     6.90 | Test Loss: 2.585 | Test PPL:    13.27 |\n",
      "| Epoch: 92 | Train Loss: 1.925 | Train PPL:     6.85 | Test Loss: 2.613 | Test PPL:    13.65 |\n",
      "| Epoch: 93 | Train Loss: 1.923 | Train PPL:     6.84 | Test Loss: 2.590 | Test PPL:    13.32 |\n",
      "| Epoch: 94 | Train Loss: 1.908 | Train PPL:     6.74 | Test Loss: 2.617 | Test PPL:    13.69 |\n",
      "| Epoch: 95 | Train Loss: 1.911 | Train PPL:     6.76 | Test Loss: 2.610 | Test PPL:    13.59 |\n",
      "| Epoch: 96 | Train Loss: 1.909 | Train PPL:     6.75 | Test Loss: 2.616 | Test PPL:    13.68 |\n",
      "| Epoch: 97 | Train Loss: 1.887 | Train PPL:     6.60 | Test Loss: 2.605 | Test PPL:    13.53 |\n",
      "| Epoch: 98 | Train Loss: 1.907 | Train PPL:     6.74 | Test Loss: 2.613 | Test PPL:    13.64 |\n",
      "| Epoch: 99 | Train Loss: 1.893 | Train PPL:     6.64 | Test Loss: 2.598 | Test PPL:    13.44 |\n",
      "| Epoch: 100 | Train Loss: 1.895 | Train PPL:     6.65 | Test Loss: 2.615 | Test PPL:    13.67 |\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir('saves'):\n",
    "    os.makedirs('saves')\n",
    "    \n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
    "    test_loss = evaluate(model, test_iter, criterion)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), f'saves/model-{PROJECT_NAME}-{EMBEDDING_INIT}.pt')    \n",
    "    \n",
    "    print_string = f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):8.2f} | Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):8.2f} |'\n",
    "    print(print_string)\n",
    "    with open(f'logs/results-{PROJECT_NAME}-{EMBEDDING_INIT}.txt', 'a') as w:\n",
    "        w.write(f'{print_string}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model = ConvAttentionNetwork(VOCAB_SIZE, EMBEDDING_DIM, K1, K2, W1, W2, W3, DROPOUT, PRELU, PAD_IDX)\\n\\nmodel = model.to(device)\\n\\nmodel.load_state_dict(torch.load('.save/model.pt'))\\n\\nmodel.eval()\\n\\nfor i, batch in enumerate(test_iter):\\n    \\n    bodies = batch.body[:,0].unsqueeze(1)\\n    names = batch.name[:,0].unsqueeze(1)\\n    \\n    output = model(bodies, names).squeeze(0)\\n    \\n    bodies = [BODY.vocab.itos[b.item()] for b in bodies.squeeze(1).cpu() if b != PAD_IDX]\\n    names = [NAME.vocab.itos[n.item()] for n in names.squeeze(1).cpu() if n != PAD_IDX][1:-1]\\n    \\n    #print(f'METHOD BODY: {bodies}')\\n    print(f'METHOD NAME: {names}')\\n    \\n    pred = [BODY.vocab.itos[o.max(1)[1].item()] for o in output][1:]\\n    \\n    \\n    print(f'PRED NAME: {pred}')\\n    print('\\n')\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = ConvAttentionNetwork(VOCAB_SIZE, EMBEDDING_DIM, K1, K2, W1, W2, W3, DROPOUT, PRELU, PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('.save/model.pt'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, batch in enumerate(test_iter):\n",
    "    \n",
    "    bodies = batch.body[:,0].unsqueeze(1)\n",
    "    names = batch.name[:,0].unsqueeze(1)\n",
    "    \n",
    "    output = model(bodies, names).squeeze(0)\n",
    "    \n",
    "    bodies = [BODY.vocab.itos[b.item()] for b in bodies.squeeze(1).cpu() if b != PAD_IDX]\n",
    "    names = [NAME.vocab.itos[n.item()] for n in names.squeeze(1).cpu() if n != PAD_IDX][1:-1]\n",
    "    \n",
    "    #print(f'METHOD BODY: {bodies}')\n",
    "    print(f'METHOD NAME: {names}')\n",
    "    \n",
    "    pred = [BODY.vocab.itos[o.max(1)[1].item()] for o in output][1:]\n",
    "    \n",
    "    \n",
    "    print(f'PRED NAME: {pred}')\n",
    "    print('\\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_precision_recall(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Get the precision/recall for the given token.\n",
    "    :param predicted_parts: a list of predicted parts\n",
    "    :param gold_set_parts: a list of the golden parts\n",
    "    :return: precision, recall, f1 as floats\n",
    "    \"\"\"\n",
    "    \n",
    "    ground_truth = y_true[:]\n",
    "    \n",
    "    tp = 0\n",
    "    for subtoken in set(y_pred):\n",
    "        if subtoken == UNK_IDX:\n",
    "            continue\n",
    "        if subtoken in ground_truth:\n",
    "            ground_truth.remove(subtoken)\n",
    "            tp += 1\n",
    "\n",
    "    assert tp <= len(y_pred), (tp, len(y_pred))\n",
    "    \n",
    "    if len(y_pred) > 0:\n",
    "        precision = float(tp) / len(y_pred)\n",
    "    else:\n",
    "        precision = 0\n",
    "\n",
    "    assert tp <= len(y_true), (y_true)\n",
    "    \n",
    "    if len(y_true) > 0:\n",
    "        recall = float(tp) / len(y_true)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.\n",
    "        \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model = ConvAttentionNetwork(VOCAB_SIZE, EMBEDDING_DIM, K1, K2, W1, W2, W3, DROPOUT, PRELU, PAD_IDX)\\n\\nmodel = model.to(device)\\n\\nmodel.load_state_dict(torch.load('.save/model.pt'))\\n\\nmodel.eval()\\n\\nprecision = 0\\nrecall = 0\\nf1 = 0\\nn_examples = 0\\n\\nfor i, batch in enumerate(test_iter):\\n    \\n    bodies = batch.body\\n    names = batch.name\\n    \\n    output = model(bodies, names)\\n\\n    preds = output.max(2)[1]\\n  \\n    examples = names.shape[1]\\n    n_examples += examples\\n    \\n    for ex in range(examples):\\n        actual = [n.item() for n in names[:,ex][1:]]\\n        predicted = [p.item() for p in preds[:,ex][1:]]\\n        _precision, _recall, _f1 = token_precision_recall(predicted, actual)\\n        precision += _precision\\n        recall += _recall\\n        f1 += _f1\\n    \\nprint(f1/n_examples)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = ConvAttentionNetwork(VOCAB_SIZE, EMBEDDING_DIM, K1, K2, W1, W2, W3, DROPOUT, PRELU, PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('.save/model.pt'))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "f1 = 0\n",
    "n_examples = 0\n",
    "\n",
    "for i, batch in enumerate(test_iter):\n",
    "    \n",
    "    bodies = batch.body\n",
    "    names = batch.name\n",
    "    \n",
    "    output = model(bodies, names)\n",
    "\n",
    "    preds = output.max(2)[1]\n",
    "  \n",
    "    examples = names.shape[1]\n",
    "    n_examples += examples\n",
    "    \n",
    "    for ex in range(examples):\n",
    "        actual = [n.item() for n in names[:,ex][1:]]\n",
    "        predicted = [p.item() for p in preds[:,ex][1:]]\n",
    "        _precision, _recall, _f1 = token_precision_recall(predicted, actual)\n",
    "        precision += _precision\n",
    "        recall += _recall\n",
    "        f1 += _f1\n",
    "    \n",
    "print(f1/n_examples)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
